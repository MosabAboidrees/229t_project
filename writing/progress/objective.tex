\subsubsection*{Understanding the Objective}

In order to gain insight into the objective function landscape of neural networks
in terms of their parameters, we ran a set of experiments with neural network models
that are comparatively smaller than those used in our optimization experiments.
Specifically, the networks we used have an input dimension of 50, an output dimension
of 10 (determined by the classification task), and hidden layers with 16 units for each
layer. Apparently this is smaller than any state-of-the art models, but here we try to 
limit the model size in order to a) efficiently train the networks for repeated runs, and
b) to save the trace of model parameters as well as gradients as the training progresses
to visualize the cost function landscape.

As we were training these small neural networks with (minibatch stochastic) gradient 
descent, we varied the number of hidden layers from zero to five,
as well as the minibatch size in $\{120,600,6000,60000\}$ where 60000 is the total 
number of training instances we have, i.e. in this setting we are essentially performing 
batch gradient descent on the neural networks. We would like to investigate while
extra hidden layers help increase the expressive power of neural networks, do they 
also make the objective function of the network more ill-defined; and as the minibatch
size shrink (relative to the size of the training set), can we still assume that 
the minibatches are well representing the landscape of the batch cost function, given
that the neural network cost function is highly nonlinear.

For each model and minibatch setting, we run gradient descent for 1,000 iterations 
(parameter updates) with a fixed learning rate of 0.1. We saved for each iteration the
parameters of the network, as well as the cost and gradient evaluated for that
(mini)batch. For each model/minibatch setting, we repeat the network training from
20 independent random initializations and plot these training traces with a technique
introduced below on the same plot to visualize the landscape of neural network cost
functions. Here we will try to justify our choice of network size for visualization 
purposes by computing the size of these trace files. For the largest model that we
considered in this part, it has 5 hidden layers of 16 units, which sums up to about
2,000 parameters. With this in mind our trace files will take tens of megabytes on disk,
which will grow nearly quadratically if we increase the hidden layer size.

After saving the training traces, we attempted to visualize the cost functions by
linear projections to lower dimensional spaces. Specifically, we computed the pricipal
components of the covariance matrix of the gradient traces across the 20 trials
for each setting, and projected the model parameters into the subspace of the leading
principle values. The logic behind this is that we want to characterize the landscape
of the cost function in the direction where the model parameters (despite initialization)
travelled the most \footnote{An tempting alternative would be to project the parameters
into the space of the principal components of the parameter's covariance, but this 
essentially contradicts our purpose, and yields uninteresting results that we choose not
to show.}. However, despite our earlier success in visualization with small random 
perturbations to the same initialization points, this does not appear to be the case 
for general random initializations (See Figure \ref{fig:cost_layer} and \ref{fig:cost_minibatch}).