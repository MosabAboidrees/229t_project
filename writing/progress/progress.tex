\documentclass[12pt,english]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx}

% Some macros for your convenience
\newcommand\bbR{\ensuremath{\mathbb{R}}} % Real numbers
\newcommand\bbZ{\ensuremath{\mathbb{Z}}} % Integers
\newcommand\bbE{\ensuremath{\mathbb{E}}} % Expectation
\DeclareMathOperator*{\tr}{tr} % Trace
\DeclareMathOperator*{\diag}{diag} % Diagonal matrix
\DeclareMathOperator*{\sign}{sign} % Sign
\DeclareMathOperator*{\var}{Var} % Variance
\DeclareMathOperator*{\cov}{Cov} % Covariance
\newcommand{\1}{\mathbb{I}} % Indicator 

\title{
{\large CS229T/STATS231 Winter 2014 -- Project Progress Report }
}

\author{ \large
Awni Hannun \\
\texttt{awni@stanford.edu}
\and
Peng Qi \\
\texttt{pengqi@stanford.edu}
}
\date{}

\begin{document}
\maketitle

\subsubsection*{Introduction}
We attempt to prescribe a stochastic optimization procedure for the typical
multilayer neural network objective. We plan to do this by developing a deeper
understanding of the most typical objective(s) and architectures used in this
setting. We apply this analysis to motivate the selection of optimization
procedures we study and attempt to give a thorough comparison of a few
competing algorithms using existing theory and empirical results. 

\subsubsection*{Experimental Setup}

In most experiments performed, we use a standard multilayer neural network with
dense connections between each layer of hidden activations. The number of
hidden layers and size of the networks vary; for the MNIST experiments
described below we use 2 hidden layers with 200 units each. The rectified
linear nonlinearity ($f(x) = \max\{.1x,x\}$) is applied at each hidden unit. Most
computation is accelerated using GPUs and the Gnumpy for python library. 

We use the MNIST handwritten digits dataset which consists of 60,000 training
images and 10,000 test images. The images are all $28\times28$ pixels; however,
prior to training most networks, we project the data onto the 100 principal
components with standard PCA and use this as the input. This is primarily done
for computational efficiency, decreasing the total number of parameters in the
network. During optimization the network is usually trained for 30 epochs
(passes over the training data), randomly permuting the order of the training
examples to the network.

\input{objective}
\input{optimization}

\subsubsection*{Plans}

Further plans for our project include:

\begin{enumerate}

\item Gain better understanding of the network objective(s) by using second
order information.  For example for small $n$, we can estimate rank $n$
approximations to the Hessian of the objective to better understand local
curvature.

\item Evaluate the same empirical studies on a slightly harder to fit dataset.
We plan to move to CIFAR-100 next.

\item Compare the NAG, AdaGrad and other variations of the two optimization
procedures to convergence rates given in the literature.

\item Give an evaluation and discussion on generalization performance.

\end{enumerate}

\subsubsection*{Appendix}

\showthe\columnwidth

\bibliography{refs}{}
\bibliographystyle{plain}

\end{document}

