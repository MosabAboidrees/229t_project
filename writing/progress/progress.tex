\documentclass[12pt,english]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx}

% Some macros for your convenience
\newcommand\bbR{\ensuremath{\mathbb{R}}} % Real numbers
\newcommand\bbZ{\ensuremath{\mathbb{Z}}} % Integers
\newcommand\bbE{\ensuremath{\mathbb{E}}} % Expectation
\DeclareMathOperator*{\tr}{tr} % Trace
\DeclareMathOperator*{\diag}{diag} % Diagonal matrix
\DeclareMathOperator*{\sign}{sign} % Sign
\DeclareMathOperator*{\var}{Var} % Variance
\DeclareMathOperator*{\cov}{Cov} % Covariance
\newcommand{\1}{\mathbb{I}} % Indicator 

\title{
{\large CS229T/STATS231 Winter 2014 -- Project Progress Report }
}

\author{ \large
Awni Hannun \\
\texttt{awni@stanford.edu}
\and
Peng Qi \\
\texttt{pengqi@stanford.edu}
}
\date{}

\begin{document}
\maketitle

\subsubsection*{Introduction}
We attempt to prescribe a stochastic optimization procedure for the typical
multilayer neural network objective. We plan to do this by developing a deeper
understanding of the most common objective(s) and architectures used in this
setting. We apply this analysis to motivate the selection of optimization
procedures we study and attempt to give a thorough comparison of a few
competing algorithms using existing theory and empirical results. 

\subsubsection*{Experimental Setup}

In most experiments performed, we use a standard multilayer neural network with
dense connections between each layer of hidden activations. The number of
hidden layers and size of the networks vary; for the MNIST experiments
described below we use 2 hidden layers with 200 units each. The leaky rectified
linear nonlinearity ($f(x) = \max\{0.01x,x\}$) is applied at each hidden unit.
Most computation is accelerated using GPUs and the Gnumpy library for Python. 

We use the MNIST handwritten digits dataset which consists of 60,000 training
images and 10,000 test images. The images are all $28\times28$ pixels; however,
prior to training most networks we project the data onto the leading principal
components (between 50 and 100) with standard PCA and use this as the input.
This primarily achieves computational efficiency by decreasing the total number
of parameters in the network. 

\input{objective}
\input{optimization}

\subsubsection*{Plans}

Further plans for our project include:

\begin{enumerate}

\item Use alternative dimensionality reduction techniques for cost function
landscape visualization, e.g. t-SNE.

\item Evaluate the same empirical studies on a slightly harder to fit (``more realistic'') dataset.
We plan to move to CIFAR-100 next.

\item Compare the NAG, AdaGrad and other variations of the two optimization
procedures to convergence rates given in the literature.

\item Gain better understanding of the network objective(s) by using second
order information.  For example for small $n$, we can estimate rank $n$
approximations to the Hessian of the objective to better understand local
curvature, and hopefully improve existing algorithms by utilizing
such information.

\item Give an evaluation and discussion on generalization performance.

\end{enumerate}

\newpage
\subsubsection*{Appendix}

\begin{figure}[h!]
{\centering
\includegraphics[width=.23\textwidth]{../plots/results_.pdf} 
\includegraphics[width=.23\textwidth]{../plots/results_16.pdf} 
\includegraphics[width=.23\textwidth]{../plots/results_16_16.pdf}
\includegraphics[width=.23\textwidth]{../plots/results_16_16_16_16_16.pdf}
}
\caption{Visualization of cost function landscapes for neural networks of hidden layer number 0, 1, 2, and 5} \label{fig:cost_layer}
\end{figure}

\begin{figure}[h!]
{\centering
\includegraphics[width=.23\textwidth]{../plots/results_16_16_16.pdf}
\includegraphics[width=.23\textwidth]{../plots/results_[6000]_16_16_16.pdf}
\includegraphics[width=.23\textwidth]{../plots/results_[600]_16_16_16.pdf}
\includegraphics[width=.23\textwidth]{../plots/results_[120]_16_16_16.pdf}
}
\caption{Visualization of cost function landscapes for neural networks of 3 hidden layers, trained with minibatche sizes 60000, 6000, 600, and 120}\label{fig:cost_minibatch}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics{../plots/allcurves.pdf}
    \caption{\label{fig:allcurves} Plot of estimated objective on the training
set over iterations of optimization.}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics{../plots/bestcurves.pdf}
    \caption{\label{fig:bestcurves} Plot of estimated objective with scaled
y-axis in order to see finer differences between the three best optimization
methods.}
\end{figure}

\bibliography{refs}{}
\bibliographystyle{plain}

\end{document}

