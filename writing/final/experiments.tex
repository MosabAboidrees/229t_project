\section{Experiments}

\subsection{Setup}
In most experiments performed, we use a standard multilayer neural network with
dense connections between each layer of hidden activations. We study two
classes of networks. The first class is the standard densely connected
classification network trained with the cross entropy loss. For a single
training input and label $(x^{(i)},y^{(i)})$, the loss is given by
\begin{align}
\ell (x^{(i)},y^{(i)}) = - \sum_{k=1}^K \1\{y^{(i)} = k\} \log p_{i,k}
\end{align}
where $K$ is the number of classes and $p_{i,k}$ is the probability the model
assigns to example $i$ taking on label $k$. We also study the autoencoder
network which attempts to reconstruct the input after nonlinearly projecting it
to a much lower dimension. We use the squared loss to train this network, which
on input $x^{(i)}$ is given by
\begin{align}
\ell(x^{(i)}) = \frac{1}{2} \|x^{(i)} - h(x^{(i)})\|_2^2
\end{align}
and $h$ is the function which maps the network input to output.

The number of hidden layers and size of the networks vary; for the MNIST and
CIFAR classification experiments described below we use between 2 - 4 hidden
layers with 200 units each. The autoencoder is only trained on MNIST, and we
use the same 1.6 million parameter, 7 hidden layer network as in
\cite{hinton_2006} which is known to be a difficult to optimize benchmark. The
leaky rectified linear nonlinearity ($f(x) = \max\{0.01x,x\}$) is applied at
each hidden unit.  Most computation is accelerated using GPUs and the Gnumpy
library for Python \cite{tielemen_2010}.

We experiment mostly with the MNIST handwritten digits dataset which consists
of 60,000 training images and 10,000 test images. The images are all
$28\times28$ pixels; however, for computational efficiency, prior to training
classification networks we project the data onto the leading principal
components (between 50 and 100) and use this as the input. We also ran
experiments on the CIFAR-10 object recognition dataset \cite{krizhevsky_2009}.
This benchmark consists of 50,000 training images and 10,000 test images all
$32 \times 32$ and RGB. We first grayscale the images then use the same
preprocessing as that of the MNIST data.

\subsection{Results}

- discussion on trying SGD, CM, NAG, AdA 
- classification curves for MNIST and CIFAR
- discussion
- curve for auto encoder and discussion
- discussion of affect of learning rate, momentum etc. Include learning rate figures in appendix.
  - NAG is affected but not nearly as much as adagrad which is not nearly as affected as decayed adagrad
- generalization error?  - plot of test error vs training error for three best (NAG, adagrad(3), decayed adagrad)
