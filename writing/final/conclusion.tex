\section{Conclusion}

In this course project, we ....

Despite our empirical analyses and experiments of different optimization techniques applied
to training deep neural networks, we do notice a lack of analyses on the generalization
properties of such techniques, which is crucial in real-world machine learning tasks. A good
generalization property accompanied with faster convergence would be strongly preferable
in training of very large deep neural networks, which are gaining strong traction. Also, it
might be worth analyzing the convergence properties of the variations of the standard
AdaGrad method at least on convex functions, which would potentially give us a better
sense of the performance properties of the modified algorithms. We propose to defer
these for future work to keep this course project succinct and coherent.