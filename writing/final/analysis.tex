\section{Analysis}

To better understand the results observed in our experiments, as well as to
provide insight into the choice of optimization methods for training deep
neural networks, we experimented with a set of smaller neural networks to study
their dynamics during training. For the purpose of visualization and expedite
computation, we PCA-whitened the images of the MNIST dataset to 50 dimensions,
and optimized classification networks with only  16 units per hidden layer. We
observed the dynamics of the network as a function the number of hidden layers
from 0 (a softmax classifier) to 5, as well as minibatch sizes in $\{120, 600,
6000, 60000\}$ where the last corresponds training with the whole training set
(batch training). Standard (stochastic) gradient descent was run for 10,000
iterations for each setting, and we compare the results.

Despite the variation in hyperparameter settings, neural networks with hidden
layers presented similar characteristics during the training process. The
average magnitude of the network parameters tends to grow rapidly in the early
stage of training and then rests in a relatively stable local minimum. By
visualizing the gradient evolution, we were able to observe more inspiring
results. Similar to the network parameters, we could also observe a clear
difference between the early and late stages of training. As the training
progresses, we can similarly observe a ``convergence'' in the distribution of
the magnitude of gradient components. However, we could see that the components
with higher magnitudes tend to switch signs more often (as observed in Figure
\ref{gradt} in the Appendix, in the gradient components with larger indices) than
those with lower magnitudes, which was almost steady (observed in components
with smaller indices). This observation seems to support the common belief
about the existence of ``ravines'' in the ladscape of deep neural network
objectives functions.  Some directions have higher curvature and thus the
parameters oscillate along these directions, while others lie in a direction
with small curvature which makes gradient-based algorithms slow to converge.
More specifically, since in our gradient visualization the gradient components
were numbered bottom-up within the network, it seems to suggest that the
parameters of the higher-level layers in the network are indeed in the
high-curvature directions, while the lower-level layers are in the opposite
regime. This could be an explanation to the common observation of ``vanishing
gradients'', and the usual difficulty in training lower layers of a deep
network.

These observations support the intuitions for the algorithms we have
experimented with, and they seem to explain the fact that the adaptive gradient
algorithms, when the step size problem is ameliorated, would outperform
accelerated gradient methods by explicitly penalizing gradient components with
high curvature and boosting the rest. From Figure \ref{adagradt} in the
Appendix, we can observe that the denominators for such adaptive gradient
methods correctly capture the curvature properties in different gradient
components, which would help to prevent the network parameters from oscillating
and travel faster along the descent direction indicated by the low-curvature
components.
